{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Before we get started, let's quickly process the dataset. The steps are:\n",
    "\n",
    "1. Download the dataset to the `data/` directory first using: `curl -O https://raw.githubusercontent.com/karpathy/makemore/master/names.txt`\n",
    "2. Open up the file and see what it looks like. It's a list of names, one per line.\n",
    "3. Generate a random split of the data into training, validation, and test sets. We'll do 1000 names each for validation and test, and the rest for training.\n",
    "4. Write each dataset as a separate file in the `data/` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  222k  100  222k    0     0  1507k      0 --:--:-- --:--:-- --:--:-- 1558k\n"
     ]
    }
   ],
   "source": [
    "!curl -o data/names.txt https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 names:\n",
      "emma\n",
      "olivia\n",
      "ava\n",
      "isabella\n",
      "sophia\n",
      "charlotte\n",
      "mia\n",
      "amelia\n",
      "harper\n",
      "evelyn\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 2. Read in all the names (N=32,032 names in total)\n",
    "names = open(\"../data/names.txt\", \"r\").readlines()\n",
    "print(\"First 10 names:\")\n",
    "for name in names[:10]:\n",
    "    print(name.strip())\n",
    "\n",
    "# 3. Get a permutation to split the names into test, val, and train sets\n",
    "random.seed(42)  # fix seed for reproducibility\n",
    "ix = list(range(len(names)))\n",
    "random.shuffle(ix)\n",
    "\n",
    "# (Validation, Test, Training) = (1000, 1000, N-2000)\n",
    "test_names = [names[i] for i in ix[:1000]]\n",
    "val_names = [names[i] for i in ix[1000:2000]]\n",
    "train_names = [names[i] for i in ix[2000:]]\n",
    "\n",
    "\n",
    "# 4. Write list of names to separate files\n",
    "def write_names(names, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for name in names:\n",
    "            f.write(name)\n",
    "\n",
    "\n",
    "write_names(test_names, \"../data/test.txt\")\n",
    "write_names(val_names, \"../data/val.txt\")\n",
    "write_names(train_names, \"../data/train.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now, we have a list of names. We need to convert them into a list of tokens (in this case, characters), so that we can train a character-level language model. More details about tokenization can be found in the [tokenization video on YouTube](https://www.youtube.com/watch?v=zduSFxRajkE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = open(\"data/train.txt\", \"r\").read()\n",
    "\n",
    "# Check that the text is as expected (a-z and a newline character)\n",
    "assert all(c == \"\\n\" or (\"a\" <= c <= \"z\") for c in train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 27\n",
      "A -> : 1\n",
      "L -> : 12\n",
      "Z -> : 26\n"
     ]
    }
   ],
   "source": [
    "# Unique characters we see in the input\n",
    "uchars = sorted(list(set(train_text)))\n",
    "vocab_size = len(uchars)\n",
    "char_to_token = {c: i for i, c in enumerate(uchars)}\n",
    "token_to_char = {i: c for i, c in enumerate(uchars)}\n",
    "# Designate \\n as the delimiting <|endoftext|> token\n",
    "EOT_TOKEN = char_to_token[\"\\n\"]\n",
    "\n",
    "print(f\"Number of unique characters: {vocab_size}\")  # This should be 27\n",
    "print(f\"A -> : {char_to_token['a']}\")\n",
    "print(f\"L -> : {char_to_token['l']}\")\n",
    "print(f\"Z -> : {char_to_token['z']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenize all the splits one time up here\n",
    "test_tokens = [char_to_token[c] for c in open(\"data/test.txt\", \"r\").read()]\n",
    "val_tokens = [char_to_token[c] for c in open(\"data/val.txt\", \"r\").read()]\n",
    "train_tokens = [char_to_token[c] for c in open(\"data/train.txt\", \"r\").read()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rayvon\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first name\n",
    "train_names[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1, 25, 22, 15, 14, 0]\n",
      "[18, 1, 25, 22, 15, 14, 0, 20, 1, 23]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first name in tokenized form\n",
    "print([char_to_token[c] for c in train_names[0]])\n",
    "\n",
    "# Look at the first few tokens of the training set\n",
    "print(train_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the first name `rayvon\\n` is tokenized into `['r', 'a', 'y', 'v', 'o', 'n', '\\n']`, and then converted into numerical indices. Here, `0` represents the newline token `\\n`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the n-gram model\n",
    "\n",
    "The central idea behind an n-gram model is to approximate the probability of a token `w_n` given the history of the previous `n` tokens (denoted by `w_{n-N+1:n-1}`, more commonly known as the \"context\"). Here, the lowercase `n` refers to the position of the token in the sequence, while the uppercase `N` refers to the length of the context. For bigram models, `N=2`, for trigram models, `N=3`, and so on.\n",
    "\n",
    "In mathematical terms, this is written as: $$ P(w*n | w*{n-N+1:n-1}) $$.\n",
    "\n",
    "The above probability is estimated using maximum likelihood estimation (MLE) as: $$ P(w*n | w*{n-N+1:n-1}) = \\frac{C(w*{n-N+1:n-1} w_n)}{C(w*{n-N+1:n-1})} $$, where `C()` denotes the count of the n-gram in the dataset.\n",
    "\n",
    "A more mathematical description can be found in the [n-gram Language Model](https://web.stanford.edu/~jurafsky/slp3/3.pdf) chapter of the book \"Speech and Language Processing\" by Dan Jurafsky & James H. Martin.\n",
    "\n",
    "Let's build one such model using the training data using `N=3`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step walkthrough of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 27, 27)\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 3\n",
    "counts = np.zeros((vocab_size,) * SEQ_LEN, dtype=np.int32)\n",
    "\n",
    "print(counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 1, 25, 22, 15, 14, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_name = [char_to_token[c] for c in train_names[0]]\n",
    "first_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have taken `N=3`, we will be building a trigram model. The model will be a dictionary where the keys are the context (a tuple of 2 tokens), and the values are also dictionaries. The inner dictionaries will have keys as the next token and values as the count of the n-gram in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18, 1, 25]\n",
      "Counts for context ['r', 'a']: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "context = first_name[:SEQ_LEN]\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "\n",
    "counts[tuple(context)] += 1\n",
    "\n",
    "print(\n",
    "    f\"Counts for context {[token_to_char[t] for t in context[:2]]}: {counts[tuple(context[:2])]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that the counts for a given context ('r a') are updated to reflect the occurrence of the token 'y' (2nd last token in the vocabulary). In other words, we are saying:\n",
    "$$ C(y | r a) = 1 $$\n",
    "\n",
    "This is the basic idea. To build a model, we just let this process run through the entire training dataset that is chunked into sizes of our sequence length `N`. Before we move on, let's see one more example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [1, 25, 22]\n",
      "Counts for context ['a', 'y']: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "context = first_name[1 : SEQ_LEN + 1]\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "\n",
    "counts[tuple(context)] += 1\n",
    "\n",
    "print(\n",
    "    f\"Counts for context {[token_to_char[t] for t in context[:2]]}: {counts[tuple(context[:2])]}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the count for the token 'v', given the context ('a y'), is increased by 1. In other words, we are saying:\n",
    "$$ C(v | a y) = 1 $$\n",
    "I hope this gives you a good intuition about how the counts are updated. Let's now build the model using the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to iterate tokens with a fixed-sized window\n",
    "def dataloader(tokens, window_size):\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        yield tokens[i : i + window_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [18, 1, 25]\n",
      "2 [1, 25, 22]\n",
      "3 [25, 22, 15]\n"
     ]
    }
   ],
   "source": [
    "# Quick example:\n",
    "example_dataloader = dataloader(train_tokens, 3)\n",
    "print(1, next(example_dataloader))\n",
    "print(2, next(example_dataloader))\n",
    "print(3, next(example_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((vocab_size,) * SEQ_LEN, dtype=np.int32)\n",
    "\n",
    "# This will iterate over all trigrams in the training set and update the counts\n",
    "for tape in dataloader(train_tokens, SEQ_LEN):\n",
    "    counts[tuple(tape)] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the next token using the model\n",
    "\n",
    "Now that we have the trained model (an array of counts), we can generate the next token given a context. This is done by calculating the probability of the occurernce of each token given the context, and then sampling from this distribution to get the next token.\n",
    "\n",
    "Let's look at an example, with context = `'a y'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference context: [1, 25]\n",
      "Counts for context (ay): [158. 361.  14.  50. 172.  74.   4.  11.  10.  36.   9.  20. 451.  55.\n",
      "  93.  47.   1.   5.  42. 129.  61.  20.  78.   2.   1.  16.  36.]\n",
      "\n",
      "Probs for context (ay): [0.08077709 0.18456033 0.00715746 0.02556237 0.08793456 0.03783231\n",
      " 0.00204499 0.00562372 0.00511247 0.01840491 0.00460123 0.01022495\n",
      " 0.2305726  0.02811861 0.04754601 0.02402863 0.00051125 0.00255624\n",
      " 0.02147239 0.06595092 0.03118609 0.01022495 0.0398773  0.00102249\n",
      " 0.00051125 0.00817996 0.01840491]\n",
      "\n",
      "Most likely next character: l\n"
     ]
    }
   ],
   "source": [
    "inference_context_char = \"ay\"\n",
    "inference_context = [char_to_token[c] for c in inference_context_char]\n",
    "print(\"Inference context:\", inference_context)\n",
    "\n",
    "inference_counts = counts[tuple(inference_context)].astype(np.float32)\n",
    "\n",
    "# Add smoothing (\"fake counts\") to all counts\n",
    "inference_counts += 1\n",
    "\n",
    "print(f\"Counts for context ({inference_context_char}): {inference_counts}\\n\")\n",
    "\n",
    "inference_counts_sum = inference_counts.sum()\n",
    "probs = inference_counts / inference_counts_sum\n",
    "\n",
    "print(f\"Probs for context ({inference_context_char}): {probs}\\n\")\n",
    "print(f\"Most likely next character: {token_to_char[probs.argmax()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at another example, with context = `'k a'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference context: [11, 1]\n",
      "Counts for context (ka): [199.  10.   4.  21.  40.  61.   1.   4.  52. 210.   6.   1. 164. 144.\n",
      "  70.   5.   6.   1. 205. 106.  91.   7.  20.  10.   2. 195.  19.]\n",
      "\n",
      "Probs for context (ka): [0.12031439 0.00604595 0.00241838 0.01269649 0.0241838  0.03688029\n",
      " 0.00060459 0.00241838 0.03143894 0.12696493 0.00362757 0.00060459\n",
      " 0.09915357 0.08706167 0.04232164 0.00302297 0.00362757 0.00060459\n",
      " 0.12394196 0.06408706 0.05501814 0.00423216 0.0120919  0.00604595\n",
      " 0.00120919 0.11789601 0.0114873 ]\n",
      "\n",
      "Most likely next character: i\n"
     ]
    }
   ],
   "source": [
    "inference_context_char = \"ka\"\n",
    "inference_context = [char_to_token[c] for c in inference_context_char]\n",
    "print(\"Inference context:\", inference_context)\n",
    "\n",
    "inference_counts = counts[tuple(inference_context)].astype(np.float32)\n",
    "\n",
    "# Add smoothing (\"fake counts\") to all counts\n",
    "inference_counts += 1\n",
    "\n",
    "print(f\"Counts for context ({inference_context_char}): {inference_counts}\\n\")\n",
    "\n",
    "inference_counts_sum = inference_counts.sum()\n",
    "probs = inference_counts / inference_counts_sum\n",
    "\n",
    "print(f\"Probs for context ({inference_context_char}): {probs}\\n\")\n",
    "print(f\"Most likely next character: {token_to_char[probs.argmax()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, given a context of `'ay'`, the model predicts `'l'` as the most likely next token, while for the context `'ka'`, the model predicts `'i'` as the most likely next token. Let's see an example of generating a sequence of tokens using the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "kayaryoce\n",
      "kr\n",
      "die\n",
      "in\n",
      "ta\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 20\n",
    "inference_context_char = \"ka\"\n",
    "\n",
    "current_context = [char_to_token[c] for c in inference_context_char]\n",
    "generated_tokens = current_context.copy()\n",
    "\n",
    "for _ in range(num_tokens):\n",
    "    inference_counts = counts[tuple(current_context)].astype(np.float32)\n",
    "    inference_counts += 1  # Add smoothing\n",
    "    probs = inference_counts / inference_counts.sum()\n",
    "\n",
    "    next_token = rng.choice(len(probs), p=probs)\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "    current_context = current_context[1:] + [\n",
    "        next_token\n",
    "    ]  # Update the context window for the next iteration\n",
    "\n",
    "generated_text = \"\".join(token_to_char[token] for token in generated_tokens)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above block of code generates a sequence of tokens given an input context. The context is used to predict the next token, which is then appended to the context. The last `N-1` tokens (in this case, 2) are used as the context for the next prediction. This process is repeated for `num_tokens` iterations to generate a sequence of tokens.\n",
    "\n",
    "However, it does not look like the model is generating meaningful names. How do we quantify this metric? We now turn to calculating the loss, and optimizing the model parameters to minimize this loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sidenote: Smoothing\n",
    "\n",
    "Smoothing involves adding a small value to all the counts in the model to ensure that no n-gram has a count of 0, which would lead to a division by zero error. The most common smoothing technique is called Laplace smoothing, where we add 1 to all the counts. See Section 3.6 of the aforementioned book for more details.\n",
    "\n",
    "_Back to understanding n-grams..._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, validation, and testing datasets\n",
    "\n",
    "It is highly recommended to understand the difference between training, validation and testing datasets. This is a topic that will be encountered in every machine learning project. A good resource in Section 3.2 of the book mentioned above for an in-depth explanation.\n",
    "\n",
    "Briefly, training data is used to train the model, validation data is used to tune hyperparameters, and test data is used to evaluate the model's performance. It is important to have a validation set in between, as using the testing data to either train the model or to tune hyperparameters can lead to overfitting and poor overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "\n",
    "Since we are building a language model to predict the next token, we want to maximize the probability of the next token given the context. In other words, if we know that the token after `'a y'` is `'v'`, we want the model to predict `'v' with high probability.\n",
    "\n",
    "Therefore, the objective is to maximize the likelihood of the next token given the context, OR equivalently, minimize the negative log likelihood of the next token given the context. This is the loss function that we will use to evaluate the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(counts, tokens, SEQ_LEN):\n",
    "    # Evaluate the given counts array on the given tokens\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for tape in dataloader(tokens, SEQ_LEN):\n",
    "        x = tape[:-1]  # The context\n",
    "        y = tape[-1]  # The actual target\n",
    "\n",
    "        # Get the probabilities from the model for this context\n",
    "        inference_counts = counts[tuple(x)].astype(np.float32)\n",
    "        inference_counts += 1\n",
    "        inference_counts_sum = inference_counts.sum()\n",
    "        probs = inference_counts / inference_counts_sum\n",
    "\n",
    "        # Get the probability for the actual target as predicted by the model\n",
    "        prob = probs[y]\n",
    "\n",
    "        # Add the negative log probability to the loss\n",
    "        sum_loss += -np.log(prob)\n",
    "\n",
    "        # Increment the count of how many contexts we've seen\n",
    "        count += 1\n",
    "\n",
    "    # Calculate the mean loss over all contexts seen\n",
    "    mean_loss = sum_loss / count if count > 0 else 0.0\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training loss on model with `N=3`: 2.2117\n",
      "Mean validation loss on model with `N=3`: 2.2521\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Mean training loss on model with `N=3`: {evaluate_model(counts, train_tokens, 3):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean validation loss on model with `N=3`: {evaluate_model(counts, val_tokens, 3):.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "With `N=3`, we get some specific loss values on the training and validation set. How do we test if this is the best value of `N`?\n",
    "\n",
    "We can try different values of `N` and see which one gives the best performance on the validation set. This process is called **hyperparameter tuning.**\n",
    "\n",
    "Let's see what happens with `N=4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 4\n",
    "counts = np.zeros((vocab_size,) * SEQ_LEN, dtype=np.int32)\n",
    "\n",
    "# This will iterate over all 5-grams (pentagram?) in the training set and update the counts\n",
    "for tape in dataloader(train_tokens, SEQ_LEN):\n",
    "    counts[tuple(tape)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for context \"ray\": [18 47  0 13 21  7  0  3  2  2  0  1 59  6 16  3  0  1  0 20  7  1  9  0\n",
      "  0  3  1]\n"
     ]
    }
   ],
   "source": [
    "print(f'Counts for context \"ray\": {counts[tuple([char_to_token[c] for c in \"ray\"])]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "karsh\n",
      "sopbpztebzszucccp\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 20\n",
    "inference_context_char = \"kar\"\n",
    "\n",
    "current_context = [char_to_token[c] for c in inference_context_char]\n",
    "generated_tokens = current_context.copy()\n",
    "\n",
    "for _ in range(num_tokens):\n",
    "    inference_counts = counts[tuple(current_context)].astype(np.float32)\n",
    "    inference_counts += 1  # Add smoothing\n",
    "    probs = inference_counts / inference_counts.sum()\n",
    "\n",
    "    next_token = rng.choice(len(probs), p=probs)\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "    current_context = current_context[1:] + [\n",
    "        next_token\n",
    "    ]  # Update the context window for the next iteration\n",
    "\n",
    "generated_text = \"\".join(token_to_char[token] for token in generated_tokens)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training loss on model with `N=4`: 2.1006\n",
      "Mean validation loss on model with `N=4`: 2.2114\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Mean training loss on model with `N=4`: {evaluate_model(counts, train_tokens, 4):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean validation loss on model with `N=4`: {evaluate_model(counts, val_tokens, 4):.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean validation loss reduced from 2.25 (`N=3`) to 2.21 (`N=4`). Similarly, we could try different values of `N` to see which one gives the best performance on the validation set. Moreover, we can treat `smoothing` as a hyperparameter and tune it as well to get the best performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search for hyperparameter tuning\n",
    "\n",
    "To perform grid-search efficiently, let's rewrite our model code in an object-oriented manner. This will allow us to easily change the hyperparameters and train the model with different values of `N` and `smoothing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    def __init__(self, vocab_size: int, seq_len: int, smoothing: float = 0.0):\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        # The same n-dimensional array of counts as before\n",
    "        self.counts = np.zeros((vocab_size,) * seq_len, dtype=np.uint32)\n",
    "\n",
    "    def train(self, tape: list):\n",
    "        assert len(tape) == self.seq_len\n",
    "\n",
    "        # Increment the count for this context\n",
    "        self.counts[tuple(tape)] += 1\n",
    "\n",
    "    def get_counts(self, tape: list):\n",
    "        assert len(tape) == self.seq_len - 1\n",
    "\n",
    "        # Get the counts for this context\n",
    "        return self.counts[tuple(tape)]\n",
    "\n",
    "    def __call__(self, tape: list):\n",
    "        assert len(tape) == self.seq_len - 1\n",
    "\n",
    "        # Get the counts, apply smoothing, and normalize to get the probabilities\n",
    "        counts = self.counts[tuple(tape)].astype(np.float32)\n",
    "\n",
    "        # Add smoothing (\"fake counts\") to all counts\n",
    "        counts += self.smoothing\n",
    "\n",
    "        counts_sum = counts.sum()\n",
    "\n",
    "        probs = counts / counts_sum\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: NgramLanguageModel, tokens: list):\n",
    "    # Evaluate the given model on the given tokens\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for tape in dataloader(tokens, model.seq_len):\n",
    "        x = tape[:-1]  # The context\n",
    "        y = tape[-1]  # The actual target\n",
    "\n",
    "        # Get the probabilities from the model for this context\n",
    "        probs = model(x)\n",
    "\n",
    "        # Get the probability for the actual target as predicted by the model\n",
    "        prob = probs[y]\n",
    "\n",
    "        # Add the negative log probability to the loss\n",
    "        sum_loss += -np.log(prob)\n",
    "\n",
    "        # Increment the count of how many contexts we've seen\n",
    "        count += 1\n",
    "\n",
    "    # Calculate the mean loss over all contexts seen\n",
    "    mean_loss = sum_loss / count if count > 0 else 0.0\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq Len : 3 | Smoothing : 0.03 | Train Loss: 2.1843 | Val Loss  : 2.2443\n",
      "Seq Len : 3 | Smoothing : 0.1 | Train Loss: 2.1870 | Val Loss  : 2.2401\n",
      "Seq Len : 3 | Smoothing : 0.3 | Train Loss: 2.1935 | Val Loss  : 2.2404\n",
      "Seq Len : 3 | Smoothing : 1.0 | Train Loss: 2.2117 | Val Loss  : 2.2521\n",
      "Seq Len : 4 | Smoothing : 0.03 | Train Loss: 1.8703 | Val Loss  : 2.1376\n",
      "Seq Len : 4 | Smoothing : 0.1 | Train Loss: 1.9028 | Val Loss  : 2.1118\n",
      "Seq Len : 4 | Smoothing : 0.3 | Train Loss: 1.9677 | Val Loss  : 2.1269\n",
      "Seq Len : 4 | Smoothing : 1.0 | Train Loss: 2.1006 | Val Loss  : 2.2114\n",
      "Seq Len : 5 | Smoothing : 0.03 | Train Loss: 1.4955 | Val Loss  : 2.3540\n",
      "Seq Len : 5 | Smoothing : 0.1 | Train Loss: 1.6335 | Val Loss  : 2.2814\n",
      "Seq Len : 5 | Smoothing : 0.3 | Train Loss: 1.8610 | Val Loss  : 2.3210\n",
      "Seq Len : 5 | Smoothing : 1.0 | Train Loss: 2.2132 | Val Loss  : 2.4903\n",
      "Seq Len : 6 | Smoothing : 0.03 | Train Loss: 1.1155 | Val Loss  : 2.7843\n",
      "Seq Len : 6 | Smoothing : 0.1 | Train Loss: 1.4304 | Val Loss  : 2.6756\n",
      "Seq Len : 6 | Smoothing : 0.3 | Train Loss: 1.8726 | Val Loss  : 2.7169\n",
      "Seq Len : 6 | Smoothing : 1.0 | Train Loss: 2.4172 | Val Loss  : 2.8761\n",
      "Best hyperparameters: {'seq_len': 4, 'smoothing': 0.1}\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [3, 4, 5, 6]\n",
    "smoothings = [0.03, 0.1, 0.3, 1.0]\n",
    "best_loss = float(\"inf\")\n",
    "best_hyperparams = {}\n",
    "\n",
    "# Now, we'll iterate over all hyperparameters, train a model, evaluate it, and keep track of the best one\n",
    "\n",
    "for seq_len, smoothing in itertools.product(seq_lens, smoothings):\n",
    "    model = NgramLanguageModel(vocab_size, seq_len, smoothing)\n",
    "\n",
    "    for tape in dataloader(train_tokens, seq_len):\n",
    "        model.train(tape)\n",
    "\n",
    "    # Calculate the training and validation loss\n",
    "    train_loss = evaluate_model(model, train_tokens)\n",
    "    val_loss = evaluate_model(model, val_tokens)\n",
    "\n",
    "    print(\n",
    "        f\"{'Seq Len':<8}: {seq_len} | {'Smoothing':<10}: {smoothing} | {'Train Loss':<10}: {train_loss:.4f} | {'Val Loss':<10}: {val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_hyperparams = {\"seq_len\": seq_len, \"smoothing\": smoothing}\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hyperparams}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the grid search, we observe that the best hyperparameters are `N=4` and `smoothing=1`. We can now train the model with these hyperparameters on the entire training dataset and evaluate it on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seq_len = best_hyperparams[\"seq_len\"]\n",
    "best_smoothing = best_hyperparams[\"smoothing\"]\n",
    "\n",
    "best_model = NgramLanguageModel(vocab_size, best_seq_len, best_smoothing)\n",
    "\n",
    "for tape in dataloader(train_tokens, best_seq_len):\n",
    "    best_model.train(tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtjixbghulys\n",
      "ikah\n",
      "aver\n",
      "malee\n",
      "claraylen\n",
      "nes\n",
      "reece\n",
      "will\n",
      "brayshaylie\n",
      "emerela\n",
      "niticwpbqzivaakiyari\n",
      "alis\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 100\n",
    "# Start with empty lines\n",
    "current_context = [EOT_TOKEN] * (best_seq_len - 1)\n",
    "generated_tokens = current_context.copy()\n",
    "\n",
    "for _ in range(num_tokens):\n",
    "    probs = best_model(current_context)\n",
    "\n",
    "    next_token = rng.choice(len(probs), p=probs)\n",
    "\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "    current_context = current_context[1:] + [\n",
    "        next_token\n",
    "    ]  # Update the context window for the next iteration\n",
    "\n",
    "    print(token_to_char[next_token], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of the generated names are sensible, but most are not. Let's look at the loss on the test set, as well as the perplexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 2.1064\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_model(best_model, test_tokens)\n",
    "print(f\"\\nTest loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 8.2184\n"
     ]
    }
   ],
   "source": [
    "perplexity = np.exp(test_loss)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we have successfully built a small n-gram language model on a dataset of names. We saw in detail how the training process works, how to generate the next token given a context, and how to evaluate the model using the loss and improve it using hyperparameter tuning. \n",
    "\n",
    "The associated `ngram.py` file contains the complete code for the model, with fixed randomness for reproducibility as well as saving the model to use later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
