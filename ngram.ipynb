{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Before we get started, let's quickly process the dataset. The steps are:\n",
    "1. Download the dataset to the `data/` directory first using: `curl -O https://raw.githubusercontent.com/karpathy/makemore/master/names.txt`\n",
    "2. Open up the file and see what it looks like. It's a list of names, one per line.\n",
    "3. Generate a random split of the data into training, validation, and test sets. We'll do 1000 names each for validation and test, and the rest for training.\n",
    "4. Write each dataset as a separate file in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  222k  100  222k    0     0   779k      0 --:--:-- --:--:-- --:--:--  795k\n"
     ]
    }
   ],
   "source": [
    "!curl -o data/names.txt https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 names:\n",
      "emma\n",
      "olivia\n",
      "ava\n",
      "isabella\n",
      "sophia\n",
      "charlotte\n",
      "mia\n",
      "amelia\n",
      "harper\n",
      "evelyn\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 2. Read in all the names (N=32,032 names in total)\n",
    "names = open(\"./data/names.txt\", \"r\").readlines()\n",
    "print(\"First 10 names:\")\n",
    "for name in names[:10]:\n",
    "    print(name.strip())\n",
    "\n",
    "# 3. Get a permutation to split the names into test, val, and train sets\n",
    "random.seed(42)  # fix seed for reproducibility\n",
    "ix = list(range(len(names)))\n",
    "random.shuffle(ix)\n",
    "\n",
    "# (Validation, Test, Training) = (1000, 1000, N-2000)\n",
    "test_names = [names[i] for i in ix[:1000]]\n",
    "val_names = [names[i] for i in ix[1000:2000]]\n",
    "train_names = [names[i] for i in ix[2000:]]\n",
    "\n",
    "\n",
    "# 4. Write list of names to separate files\n",
    "def write_names(names, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for name in names:\n",
    "            f.write(name)\n",
    "\n",
    "\n",
    "write_names(test_names, \"./data/test.txt\")\n",
    "write_names(val_names, \"./data/val.txt\")\n",
    "write_names(train_names, \"./data/train.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now, we have a list of names. We need to convert them into a list of tokens (in this case, characters), so that we can train a character-level language model. More details about tokenization can be found in the [tokenization video on YouTube](https://www.youtube.com/watch?v=zduSFxRajkE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = open(\"data/train.txt\", \"r\").read()\n",
    "\n",
    "# Check that the text is as expected (a-z and a newline character)\n",
    "assert all(c == \"\\n\" or (\"a\" <= c <= \"z\") for c in train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 27\n",
      "A -> : 1\n",
      "L -> : 12\n",
      "Z -> : 26\n"
     ]
    }
   ],
   "source": [
    "# Unique characters we see in the input\n",
    "uchars = sorted(list(set(train_text)))\n",
    "vocab_size = len(uchars)\n",
    "char_to_token = {c: i for i, c in enumerate(uchars)}\n",
    "token_to_char = {i: c for i, c in enumerate(uchars)}\n",
    "# Designate \\n as the delimiting <|endoftext|> token\n",
    "EOT_TOKEN = char_to_token[\"\\n\"]\n",
    "\n",
    "print(f\"Number of unique characters: {vocab_size}\")  # This should be 27\n",
    "print(f\"A -> : {char_to_token['a']}\")\n",
    "print(f\"L -> : {char_to_token['l']}\")\n",
    "print(f\"Z -> : {char_to_token['z']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenize all the splits one time up here\n",
    "test_tokens = [char_to_token[c] for c in open(\"data/test.txt\", \"r\").read()]\n",
    "val_tokens = [char_to_token[c] for c in open(\"data/val.txt\", \"r\").read()]\n",
    "train_tokens = [char_to_token[c] for c in open(\"data/train.txt\", \"r\").read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rayvon\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first name\n",
    "train_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 1, 25, 22, 15, 14, 0]\n",
      "[18, 1, 25, 22, 15, 14, 0, 20, 1, 23]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first name in tokenized form\n",
    "print([char_to_token[c] for c in train_names[0]])\n",
    "\n",
    "# Look at the first few tokens of the training set\n",
    "print(train_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the first name `rayvon\\n` is tokenized into `['r', 'a', 'y', 'v', 'o', 'n', '\\n']`, and then converted into numerical indices. Here, `0` represents the newline token `\\n`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the n-gram model\n",
    "\n",
    "The central idea behind an n-gram model is to approximate the probability of a token `w_n` given the history of the previous `n` tokens (denoted by `w_{n-N+1:n-1}`, more commonly known as the \"context\"). Here, the lowercase `n` refers to the position of the token in the sequence, while the uppercase `N` refers to the length of the context. For bigram models, `N=2`, for trigram models, `N=3`, and so on.\n",
    "\n",
    "In mathematical terms, this is written as: $$ P(w_n | w_{n-N+1:n-1}) $$.\n",
    "\n",
    "The above probability is estimated using maximum likelihood estimation (MLE) as: $$ P(w_n | w_{n-N+1:n-1}) = \\frac{C(w_{n-N+1:n-1} w_n)}{C(w_{n-N+1:n-1})} $$, where `C()` denotes the count of the n-gram in the dataset.\n",
    "\n",
    "A more mathematical description can be found in the [n-gram Language Model](https://web.stanford.edu/~jurafsky/slp3/3.pdf) chapter of the book \"Speech and Language Processing\" by Dan Jurafsky & James H. Martin.\n",
    "\n",
    "Let's build one such model using the training data using `N=3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step walkthrough of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 27, 27)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 3\n",
    "counts = np.zeros((vocab_size,) * seq_len, dtype=np.int32)\n",
    "\n",
    "print(counts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 1, 25, 22, 15, 14, 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_name = [char_to_token[c] for c in train_names[0]]\n",
    "first_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have taken `N=3`, we will be building a trigram model. The model will be a dictionary where the keys are the context (a tuple of 2 tokens), and the values are also dictionaries. The inner dictionaries will have keys as the next token and values as the count of the n-gram in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18, 1, 25]\n",
      "Counts for context ['r', 'a']: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "context = first_name[:seq_len]\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "\n",
    "counts[tuple(context)] += 1\n",
    "\n",
    "print(f'Counts for context {[token_to_char[t] for t in context[:2]]}: {counts[tuple(context[:2])]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that the counts for a given context ('r a') are updated to reflect the occurrence of the token 'y' (2nd last token in the vocabulary). In other words, we are saying: \n",
    "$$ C(y | r a) = 1 $$\n",
    "\n",
    "This is the basic idea. To build a model, we just let this process run through the entire training dataset that is chunked into sizes of our sequence length `N`. Before we move on, let's see one more example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [1, 25, 22]\n",
      "Counts for context ['a', 'y']: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "context = first_name[1 : seq_len + 1]\n",
    "\n",
    "print(f\"Context: {context}\")\n",
    "\n",
    "counts[tuple(context)] += 1\n",
    "\n",
    "print(\n",
    "    f\"Counts for context {[token_to_char[t] for t in context[:2]]}: {counts[tuple(context[:2])]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the count for the token 'v', given the context ('a y'), is increased by 1. In other words, we are saying: \n",
    "$$ C(v | a y) = 1 $$ \n",
    "I hope this gives you a good intuition about how the counts are updated. Let's now build the model using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to iterate tokens with a fixed-sized window\n",
    "def dataloader(tokens, window_size):\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        yield tokens[i : i + window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [18, 1, 25]\n",
      "2 [1, 25, 22]\n",
      "3 [25, 22, 15]\n"
     ]
    }
   ],
   "source": [
    "# Quick example:\n",
    "example_dataloader = dataloader(train_tokens, 3)\n",
    "print(1, next(example_dataloader))\n",
    "print(2, next(example_dataloader))\n",
    "print(3, next(example_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((vocab_size,) * seq_len, dtype=np.int32)\n",
    "\n",
    "# This will iterate over all trigrams in the training set and update the counts\n",
    "for tape in dataloader(train_tokens, seq_len):\n",
    "    counts[tuple(tape)] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the next token using the model\n",
    "\n",
    "Now that we have the trained model (an array of counts), we can generate the next token given a context. This is done by calculating the probability of the occurernce of each token given the context, and then sampling from this distribution to get the next token.\n",
    "\n",
    "Let's look at an example, with context = `'a y'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference context: [1, 25]\n",
      "Counts for context (ay): [158. 361.  14.  50. 172.  74.   4.  11.  10.  36.   9.  20. 451.  55.\n",
      "  93.  47.   1.   5.  42. 129.  61.  20.  78.   2.   1.  16.  36.]\n",
      "\n",
      "Probs for context (ay): [0.08077709 0.18456033 0.00715746 0.02556237 0.08793456 0.03783231\n",
      " 0.00204499 0.00562372 0.00511247 0.01840491 0.00460123 0.01022495\n",
      " 0.2305726  0.02811861 0.04754601 0.02402863 0.00051125 0.00255624\n",
      " 0.02147239 0.06595092 0.03118609 0.01022495 0.0398773  0.00102249\n",
      " 0.00051125 0.00817996 0.01840491]\n",
      "\n",
      "Most likely next character: l\n"
     ]
    }
   ],
   "source": [
    "inference_context_char = \"ay\"\n",
    "inference_context = [char_to_token[c] for c in inference_context_char]\n",
    "print(\"Inference context:\", inference_context)\n",
    "\n",
    "inference_counts = counts[tuple(inference_context)].astype(np.float32)\n",
    "\n",
    "# Add smoothing (\"fake counts\") to all counts\n",
    "inference_counts += 1\n",
    "\n",
    "print(f\"Counts for context ({inference_context_char}): {inference_counts}\\n\")\n",
    "\n",
    "inference_counts_sum = inference_counts.sum()\n",
    "probs = inference_counts / inference_counts_sum\n",
    "\n",
    "print(f\"Probs for context ({inference_context_char}): {probs}\\n\")\n",
    "print(f\"Most likely next character: {token_to_char[probs.argmax()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at another example, with context = `'k a'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference context: [11, 1]\n",
      "Counts for context (ka): [199.  10.   4.  21.  40.  61.   1.   4.  52. 210.   6.   1. 164. 144.\n",
      "  70.   5.   6.   1. 205. 106.  91.   7.  20.  10.   2. 195.  19.]\n",
      "\n",
      "Probs for context (ka): [0.12031439 0.00604595 0.00241838 0.01269649 0.0241838  0.03688029\n",
      " 0.00060459 0.00241838 0.03143894 0.12696493 0.00362757 0.00060459\n",
      " 0.09915357 0.08706167 0.04232164 0.00302297 0.00362757 0.00060459\n",
      " 0.12394196 0.06408706 0.05501814 0.00423216 0.0120919  0.00604595\n",
      " 0.00120919 0.11789601 0.0114873 ]\n",
      "\n",
      "Most likely next character: i\n"
     ]
    }
   ],
   "source": [
    "inference_context_char = \"ka\"\n",
    "inference_context = [char_to_token[c] for c in inference_context_char]\n",
    "print(\"Inference context:\", inference_context)\n",
    "\n",
    "inference_counts = counts[tuple(inference_context)].astype(np.float32)\n",
    "\n",
    "# Add smoothing (\"fake counts\") to all counts\n",
    "inference_counts += 1\n",
    "\n",
    "print(f\"Counts for context ({inference_context_char}): {inference_counts}\\n\")\n",
    "\n",
    "inference_counts_sum = inference_counts.sum()\n",
    "probs = inference_counts / inference_counts_sum\n",
    "\n",
    "print(f\"Probs for context ({inference_context_char}): {probs}\\n\")\n",
    "print(f\"Most likely next character: {token_to_char[probs.argmax()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sidenote: Smoothing\n",
    "\n",
    "Smoothing involves adding a small value to all the counts in the model to ensure that no n-gram has a count of 0, which would lead to a division by zero error. The most common smoothing technique is called Laplace smoothing, where we add 1 to all the counts. See Section 3.6 of the aforementioned book for more details.\n",
    "\n",
    "*Back to understanding n-grams...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, given a context of `'ay'`, the model predicts `'l'` as the most likely next token, while for the context `'ka'`, the model predicts `'i'` as the most likely next token.\n",
    "\n",
    "How do we evaluate if this is correct?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
